



Difference between virtual machines and container?

Architecture:
Virtual Machines (VMs): A virtual machine emulates a complete hardware environment, including a virtual CPU, memory, storage, and networking interfaces. Each VM runs its own operating system (OS), known as the guest OS, on top of a hypervisor installed on the host hardware. The hypervisor is responsible for managing and allocating resources to VMs.
Containers: Containers provide a lightweight form of virtualization where applications and their dependencies are packaged together along with the minimal set of OS libraries and binaries needed to run. Containers share the host OS kernel and resources, but they are isolated from one another using namespaces and cgroups, providing process-level isolation.

Resource Utilization:
Virtual Machines (VMs): VMs consume more resources because they include a complete OS for each instance. They have higher overhead in terms of memory and disk space since they run multiple OS kernels.
Containers: Containers are lightweight and consume fewer resources compared to VMs because they share the host OS kernel and libraries. They have lower overhead and faster startup times, making them more efficient for running multiple instances on the same host.

Isolation:
Virtual Machines (VMs): VMs provide strong isolation between instances because each VM has its own OS kernel. This isolation makes VMs more secure but comes with higher resource overhead.
Containers: Containers provide process-level isolation using kernel namespaces and control groups (cgroups). While containers offer good isolation for most use cases, they may not provide the same level of security and isolation as VMs in certain scenarios.

Management:
Virtual Machines (VMs): Managing VMs involves provisioning, configuring, and maintaining complete OS instances. VM management typically requires more effort and time compared to containers.
Containers: Containers are easier to manage and deploy since they encapsulate applications and their dependencies into a single package. Container orchestration platforms like Kubernetes simplify container management, scaling, and orchestration across clusters of hosts.

Which is Better?
The choice between containers and virtual machines depends on the specific use case and requirements:
VMs are suitable for scenarios requiring strong isolation, compatibility with legacy applications, and support for multiple operating systems.
Containers are ideal for microservices architectures, cloud-native applications, and environments where resource efficiency, scalability, and fast deployment are critical.



Master Components:

API Server: The central management point for the Kubernetes cluster. It exposes the Kubernetes API, which clients (like kubectl) use to interact with the cluster.
Scheduler: Watches for newly created pods with no assigned node and selects a node for them to run on based on resource availability and other constraints.
Controller Manager: Runs controller processes that regulate the state of the cluster, such as node and pod auto-scaling, endpoints, etc.
etcd: A distributed key-value store that stores the cluster's configuration data, state, and metadata. It serves as the cluster's backing store.


Node Components:

Kubelet: An agent that runs on each node in the cluster. It ensures that containers are running in a pod by managing the pod's lifecycle, monitoring its health, and reporting back to the API server.
Kube-proxy: Maintains network rules on nodes. It handles routing traffic to the appropriate pod based on IP address and port number, load balancing across multiple pods, and providing service discovery.
Container Runtime: The software responsible for running containers, such as Docker, containerd, or CRI-O. It pulls images from container registries, runs containers, and manages their lifecycle.


Add-Ons:

DNS: Provides DNS-based service discovery for Kubernetes services.
Dashboard: A web-based UI for managing and monitoring Kubernetes clusters.
Ingress Controller: Manages external access to services in a cluster, typically by providing HTTP and HTTPS routing.
Networking:

Pod Network: A network overlay that enables communication between pods across nodes in the cluster. Common pod networking solutions include Calico, Flannel, and Cilium.


Deploy K8S in MAC:
curl -LO "
https://dl.k8s.io/release/$(curl
-L -s
https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl.sha256"
echo "$(cat kubectl.sha256)  kubectl" | shasum -a 256 --check
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl
kubectl version --client
GO111MODULE="on" go install sigs.k8s.io/kind@v0.11.1\n
kind create cluster --name my-cluster --image=kindest/node:v1.21.1 --container-runtime=podman
sudo mv kind-linux-amd64 /usr/local/bin/kind
sudo mv kind /usr/local/bin/kind
kind create cluster --name my-cluster --image=kindest/node:v1.21.1 --container-runtime=podman
kind create cluster --help
kind create cluster --name my-cluster --image=kindest/node:v1.21.1
kubectl cluster-info --context kind-my-cluster
kubectl get pods
kubectl get nodes



Production-Grade Kubernetes Cluster: kubeadm
The kubeadm toolbox acts as a bootstrap for creating a production-grade Kubernetes cluster, making a single master node with an etcd configuration. Here, Docker is the cluster’s container runtime, and you can refer to our  Docker installation steps above.

Assuming you’ve successfully installed Docker, let’s build the cluster using the kubeadm toolbox.

Add the Kubernetes repo GPG key into the key manager:

# curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

Add the Kubernetes repo details to the repository source list:

# echo “deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main” | sudo tee /etc/apt/sources.list.d/kubernetes.list

Here, the installation comes in three pieces: kubelet, kubeadm, and kubectl. Let’s update the repository and start the installation:

# apt update -y

# apt-get install -y kubelet kubeadm kubectl

To hold the installed packages with current versions, use the command below. These packages cannot be upgraded or removed until the hold mark is removed.

# apt-mark hold kubelet kubeadm kubectl

Enable the process to start on boot and reload the kubelet daemon. Check the current status of the daemon:

# systemctl enable kubelet

# systemctl daemon-reload

# systemctl restart kubelet.service

kubeadm installation is now complete. Let’s create the new cluster using kubeadm init:

# kubeadm init

[init] Using Kubernetes version: v1.23.1

[preflight] Running pre-flight checks

[preflight] Pulling images required for setting up a Kubernetes cluster

…

… output truncated …

…

[addons] Applied essential addon: CoreDNS

[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

# mkdir -p $HOME/.kube

# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

# sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

# export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.

Run “kubectl apply -f [podnetwork].yaml”  

We often use Calico as the cluster’s network provider, as shown here:

# kubectl create -f https://docs.projectcalico.org/v3.15/manifests/calico.yaml

configmap/calico-config

…

… output truncated …

…

createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers

createdserviceaccount/calico-kube-controllers created

Now, we can join many worker nodes by running the following join command on each worker node as root:

# kubeadm join 10.190.0.2:6443 –token ls7ryl.mlrc2oaoob15gbog –discovery-token-ca-cert-hash sha256:13aa6e021229373bfdceb8537ab50056c7fb6b4b67435ad165ceb1b5131c9dfc

[preflight] Running pre-flight checks

[preflight] Reading configuration from the cluster…

…

… output truncated …

…

[kubelet-start] Starting the kubelet

[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap…

This node has joined the cluster:

* Certificate signing request was sent to apiserver and a response was received.

* The Kubelet was informed of the new secure connection details.

Let’s execute the below command to check the node joining status on the cluster:

# kubectl get nodes

NAME            STATUS     ROLES                 AGE   VERSION

iod-master      Ready      control-plane,master   44h   v1.23.1

iod-worker-2   NotReady   <none>                 32h   v1.23.1

iod-worker-3   Ready       <none>                32s   v1.23.1
